##版印刷株式会社 くずし字認識チャレンジ②  Memo 

Preprocessing

```bash

1. Input image is horizontal, so it should be rotated 90 counter clockwise

2. Calculating aspect ratio of all rotated image (Calculate dataset aspect ratio)
np.percentile(ratios, 80) = 20.494
image is very long so OCR model should be trained on image with similar same aspect ratio
    -> an aspect ratio = 25 is choosen

3. Preparing character dictionary (use the crtDictionary.ipynb)
We will find nearly 2000 new character. 
```

Prepraring dataset (train, val)

```text
1. Download train images from https://signate.jp/competitions/581/data

We will have totally 47k images. The (number of image) / total characters value is very low, so we use 
the following strategy for generating dataset

2. All rotated images are used as validation
Train images are generated by genImgAug. The Augmentation code are copied from PaddleOCR. 
For each validated image, 40 synthetic image was generated. 

```

Selecting framework

```bash
PaddleOCR, mmOcr are selected.
These 2 libraries are very easy to use, well-document and support a lot of ocr algorithm

```

Testing algorithm

```bash
1. PaddleOCR : CRNN, SVTR, 

2. mmOCR : CRNN, satrn

```

Result

```bash

CRNN : 0.0189154 	0.8685803 
SATRN : 0.2875670 	0.9088303 
SVTR : 0.0445458 	0.8711443 
```

Lesson learn

```text
1. too much time was spend on finding, reading about new algorithm.  
As a result, I run out off time to train the SVTR algorithm. 

-> new ocr algorithm in paddleOCr, mmOCR are added frequently

-> ocr algorithm performance can be improved by changing the basebone network into deeper one, change the input size

-> unfortunately, network that uses transformer doesn't work that way. 

2. When training CRNN and SATRN, first directory of images (all : 0~43, selected : 1~43) are excluded so in total the accuracy are a litte low.

-> want to try again but run out of time


```

End. 


